{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Security and Compliance in MLOps with Airflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "1. Set Up Airflow and Enable RBAC\n",
    "\n",
    "2. Implement Data Protection Measures\n",
    "\n",
    "3. Secure Model Artifacts\n",
    "\n",
    "4. Enable Audit Logging\n",
    "\n",
    "5. Apply Data Retention Policies\n",
    "\n",
    "6. Test and Verify the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Set Up Airflow and Enable RBAC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable RBAC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open the airflow.cfg file and set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbac = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart the Airflow webserver and scheduler.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to Create Roles and Assign Permissions in Airflow\n",
    "\n",
    "## Create Roles and Assign Permissions:\n",
    "1. Navigate to the **Security > Roles** section in the Airflow UI.\n",
    "2. Create roles (e.g., `data_scientist`, `admin`, `auditor`).\n",
    "3. Assign permissions to roles (e.g., `can_read`, `can_edit`, `can_delete`).\n",
    "\n",
    "## Assign Users to Roles:\n",
    "1. Go to the **Security > Users** section.\n",
    "2. Create users and assign them to the desired roles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Implement Data Protection Measures\n",
    "\n",
    "## Encrypt Sensitive Data:\n",
    "- Utilize the `cryptography` library to encrypt sensitive data (e.g., the target column) during the data ingestion process.\n",
    "\n",
    "## Mask Sensitive Data:\n",
    "- Replace sensitive data with placeholders during the preprocessing stage to ensure data privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import os\n",
    "from cryptography.fernet import Fernet\n",
    "import logging\n",
    "\n",
    "# Ensure the necessary directories exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Step 1: Data Ingestion with Encryption\n",
    "def data_ingestion():\n",
    "    print(\"Starting data ingestion...\")\n",
    "    data = load_breast_cancer()\n",
    "    df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    df['target'] = data.target\n",
    "\n",
    "    # Encrypt sensitive data (e.g., target column)\n",
    "    key = Fernet.generate_key()\n",
    "    cipher_suite = Fernet(key)\n",
    "    df['target'] = df['target'].apply(lambda x: cipher_suite.encrypt(str(x).encode()))\n",
    "    df.to_csv('data/encrypted_breast_cancer.csv', index=False)\n",
    "    print(\"Data ingestion completed. Encrypted data saved to 'data/encrypted_breast_cancer.csv'.\")\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "def data_preprocessing():\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    df = pd.read_csv('data/encrypted_breast_cancer.csv')\n",
    "\n",
    "    # Decrypt the target column\n",
    "    key = Fernet.generate_key()\n",
    "    cipher_suite = Fernet(key)\n",
    "    df['target'] = df['target'].apply(lambda x: int(cipher_suite.decrypt(eval(x)).decode()))\n",
    "\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Save preprocessed data\n",
    "    pd.DataFrame(X_train).to_csv('data/X_train.csv', index=False)\n",
    "    pd.DataFrame(X_test).to_csv('data/X_test.csv', index=False)\n",
    "    pd.DataFrame(y_train, columns=['target']).to_csv('data/y_train.csv', index=False)\n",
    "    pd.DataFrame(y_test, columns=['target']).to_csv('data/y_test.csv', index=False)\n",
    "\n",
    "    # Save the scaler\n",
    "    joblib.dump(scaler, 'results/scaler.pkl')\n",
    "    print(\"Data preprocessing completed. Preprocessed data and scaler saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Secure Model Artifacts\n",
    "\n",
    "## Encrypt Model Files:\n",
    "- Encrypt the trained model before saving it to disk to ensure confidentiality and prevent unauthorized access.\n",
    "\n",
    "## Sign Model Artifacts:\n",
    "- Use digital signatures to validate the integrity of model artifacts and ensure they have not been tampered with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Model Training with Encryption\n",
    "def model_training():\n",
    "    print(\"Starting model training...\")\n",
    "    X_train = pd.read_csv('data/X_train.csv')\n",
    "    y_train = pd.read_csv('data/y_train.csv')\n",
    "\n",
    "    # Convert y_train to a 1D array\n",
    "    y_train = y_train.values.ravel()\n",
    "\n",
    "    # Train the model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Encrypt and save the trained model\n",
    "    key = Fernet.generate_key()\n",
    "    cipher_suite = Fernet(key)\n",
    "    joblib.dump(model, 'results/model.pkl')\n",
    "    with open('results/model.pkl', 'rb') as f:\n",
    "        model_data = f.read()\n",
    "    encrypted_model = cipher_suite.encrypt(model_data)\n",
    "    with open('results/encrypted_model.pkl', 'wb') as f:\n",
    "        f.write(encrypted_model)\n",
    "    print(\"Model training completed. Encrypted model saved to 'results/encrypted_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Enable Audit Logging\n",
    "\n",
    "## Log Actions in the Pipeline:\n",
    "- Use Pythonâ€™s `logging` module to log actions and events in the ML pipeline for better traceability.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Model Evaluation with Logging\n",
    "def model_evaluation():\n",
    "    print(\"Starting model evaluation...\")\n",
    "    X_test = pd.read_csv('data/X_test.csv')\n",
    "    y_test = pd.read_csv('data/y_test.csv')\n",
    "\n",
    "    # Convert y_test to a 1D array\n",
    "    y_test = y_test.values.ravel()\n",
    "\n",
    "    # Load the model\n",
    "    key = Fernet.generate_key()\n",
    "    cipher_suite = Fernet(key)\n",
    "    with open('results/encrypted_model.pkl', 'rb') as f:\n",
    "        encrypted_model = f.read()\n",
    "    decrypted_model = cipher_suite.decrypt(encrypted_model)\n",
    "    with open('results/decrypted_model.pkl', 'wb') as f:\n",
    "        f.write(decrypted_model)\n",
    "    model = joblib.load('results/decrypted_model.pkl')\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model Accuracy: {accuracy}\")\n",
    "\n",
    "    # Log the accuracy\n",
    "    logging.basicConfig(filename='pipeline_audit.log', level=logging.INFO)\n",
    "    logging.info(f\"Model Accuracy: {accuracy}\")\n",
    "\n",
    "    # Save the accuracy result\n",
    "    with open('results/accuracy.txt', 'w') as f:\n",
    "        f.write(f\"Accuracy: {accuracy}\")\n",
    "    print(\"Model evaluation completed. Accuracy saved to 'results/accuracy.txt'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Apply Data Retention Policies\n",
    "\n",
    "## Delete Old Files:\n",
    "- Implement a Python function to delete old files after a specified retention period to ensure compliance with data retention policies.\n",
    "\n",
    "## Schedule the Cleanup Task:\n",
    "- Add the cleanup task to the DAG and configure it to run periodically as part of the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def delete_old_files(directory, days):\n",
    "    current_time = time.time()\n",
    "    for file in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        if os.path.getmtime(file_path) < current_time - days * 86400:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted {file_path}\")\n",
    "\n",
    "delete_old_files('data', days=30)  # Delete files older than 30 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Test and Verify the Pipeline\n",
    "\n",
    "## Trigger the DAG:\n",
    "- Use the Airflow UI to manually trigger the DAG and monitor its execution in real-time.\n",
    "## View Logs in the Airflow UI:\n",
    "- Navigate to the **Browse > Task Logs** section in the Airflow UI to view detailed logs for each task.\n",
    "\n",
    "## Verify Security and Compliance Measures:\n",
    "- Review logs to confirm that:\n",
    "  - Encryption is applied correctly.\n",
    "  - Data masking is performed as expected.\n",
    "  - Audit logging is capturing all necessary actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from airflow import DAG\\nfrom airflow.operators.python_operator import PythonOperator\\nfrom datetime import datetime\\nimport pandas as pd\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score\\nimport joblib\\nimport os\\nfrom cryptography.fernet import Fernet\\nimport logging\\n\\n# Ensure the necessary directories exist\\nos.makedirs(\\'data\\', exist_ok=True)\\nos.makedirs(\\'results\\', exist_ok=True)\\n\\n# Step 1: Data Ingestion with Encryption\\ndef data_ingestion():\\n    print(\"Starting data ingestion...\")\\n    data = load_breast_cancer()\\n    df = pd.DataFrame(data.data, columns=data.feature_names)\\n    df[\\'target\\'] = data.target\\n\\n    # Encrypt sensitive data (e.g., target column)\\n    key = Fernet.generate_key()\\n    cipher_suite = Fernet(key)\\n    df[\\'target\\'] = df[\\'target\\'].apply(lambda x: cipher_suite.encrypt(str(x).encode()))\\n    df.to_csv(\\'data/encrypted_breast_cancer.csv\\', index=False)\\n    print(\"Data ingestion completed. Encrypted data saved to \\'data/encrypted_breast_cancer.csv\\'.\")\\n\\n# Step 2: Data Preprocessing\\ndef data_preprocessing():\\n    print(\"Starting data preprocessing...\")\\n    df = pd.read_csv(\\'data/encrypted_breast_cancer.csv\\')\\n\\n    # Decrypt the target column\\n    key = Fernet.generate_key()\\n    cipher_suite = Fernet(key)\\n    df[\\'target\\'] = df[\\'target\\'].apply(lambda x: int(cipher_suite.decrypt(eval(x)).decode()))\\n\\n    X = df.drop(\\'target\\', axis=1)\\n    y = df[\\'target\\']\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n    scaler = StandardScaler()\\n    X_train = scaler.fit_transform(X_train)\\n    X_test = scaler.transform(X_test)\\n\\n    # Save preprocessed data\\n    pd.DataFrame(X_train).to_csv(\\'data/X_train.csv\\', index=False)\\n    pd.DataFrame(X_test).to_csv(\\'data/X_test.csv\\', index=False)\\n    pd.DataFrame(y_train, columns=[\\'target\\']).to_csv(\\'data/y_train.csv\\', index=False)\\n    pd.DataFrame(y_test, columns=[\\'target\\']).to_csv(\\'data/y_test.csv\\', index=False)\\n\\n    # Save the scaler\\n    joblib.dump(scaler, \\'results/scaler.pkl\\')\\n    print(\"Data preprocessing completed. Preprocessed data and scaler saved.\")\\n\\n# Step 3: Model Training with Encryption\\ndef model_training():\\n    print(\"Starting model training...\")\\n    X_train = pd.read_csv(\\'data/X_train.csv\\')\\n    y_train = pd.read_csv(\\'data/y_train.csv\\')\\n\\n    # Convert y_train to a 1D array\\n    y_train = y_train.values.ravel()\\n\\n    # Train the model\\n    model = LogisticRegression()\\n    model.fit(X_train, y_train)\\n\\n    # Encrypt and save the trained model\\n    key = Fernet.generate_key()\\n    cipher_suite = Fernet(key)\\n    joblib.dump(model, \\'results/model.pkl\\')\\n    with open(\\'results/model.pkl\\', \\'rb\\') as f:\\n        model_data = f.read()\\n    encrypted_model = cipher_suite.encrypt(model_data)\\n    with open(\\'results/encrypted_model.pkl\\', \\'wb\\') as f:\\n        f.write(encrypted_model)\\n    print(\"Model training completed. Encrypted model saved to \\'results/encrypted_model.pkl\\'.\")\\n\\n# Step 4: Model Evaluation with Logging\\ndef model_evaluation():\\n    print(\"Starting model evaluation...\")\\n    X_test = pd.read_csv(\\'data/X_test.csv\\')\\n    y_test = pd.read_csv(\\'data/y_test.csv\\')\\n\\n    # Convert y_test to a 1D array\\n    y_test = y_test.values.ravel()\\n\\n    # Load the model\\n    key = Fernet.generate_key()\\n    cipher_suite = Fernet(key)\\n    with open(\\'results/encrypted_model.pkl\\', \\'rb\\') as f:\\n        encrypted_model = f.read()\\n    decrypted_model = cipher_suite.decrypt(encrypted_model)\\n    with open(\\'results/decrypted_model.pkl\\', \\'wb\\') as f:\\n        f.write(decrypted_model)\\n    model = joblib.load(\\'results/decrypted_model.pkl\\')\\n\\n    # Make predictions\\n    y_pred = model.predict(X_test)\\n    accuracy = accuracy_score(y_test, y_pred)\\n    print(f\"Model Accuracy: {accuracy}\")\\n\\n    # Log the accuracy\\n    logging.basicConfig(filename=\\'pipeline_audit.log\\', level=logging.INFO)\\n    logging.info(f\"Model Accuracy: {accuracy}\")\\n\\n    # Save the accuracy result\\n    with open(\\'results/accuracy.txt\\', \\'w\\') as f:\\n        f.write(f\"Accuracy: {accuracy}\")\\n    print(\"Model evaluation completed. Accuracy saved to \\'results/accuracy.txt\\'.\")\\n\\n# Define the DAG\\ndefault_args = {\\n    \\'owner\\': \\'airflow\\',\\n    \\'start_date\\': datetime(2023, 10, 1),\\n    \\'retries\\': 1,\\n}\\n\\ndag = DAG(\\n    \\'ml_pipeline_dag\\',\\n    default_args=default_args,\\n    description=\\'End-to-End ML Pipeline with Security and Compliance\\',\\n    schedule_interval=\\'@daily\\',  # Run the DAG daily\\n    catchup=False,  # Disable catchup to avoid backfilling\\n)\\n\\n# Define tasks\\ningestion_task = PythonOperator(\\n    task_id=\\'data_ingestion\\',\\n    python_callable=data_ingestion,\\n    dag=dag,\\n)\\n\\npreprocessing_task = PythonOperator(\\n    task_id=\\'data_preprocessing\\',\\n    python_callable=data_preprocessing,\\n    dag=dag,\\n)\\n\\ntraining_task = PythonOperator(\\n    task_id=\\'model_training\\',\\n    python_callable=model_training,\\n    dag=dag,\\n)\\n\\nevaluation_task = PythonOperator(\\n    task_id=\\'model_evaluation\\',\\n    python_callable=model_evaluation,\\n    dag=dag,\\n)\\n\\n# Define task dependencies\\ningestion_task >> preprocessing_task >> training_task >> evaluation_task\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import os\n",
    "from cryptography.fernet import Fernet\n",
    "import logging\n",
    "\n",
    "# Ensure the necessary directories exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Step 1: Data Ingestion with Encryption\n",
    "def data_ingestion():\n",
    "    print(\"Starting data ingestion...\")\n",
    "    data = load_breast_cancer()\n",
    "    df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    df['target'] = data.target\n",
    "\n",
    "    # Encrypt sensitive data (e.g., target column)\n",
    "    key = Fernet.generate_key()\n",
    "    cipher_suite = Fernet(key)\n",
    "    df['target'] = df['target'].apply(lambda x: cipher_suite.encrypt(str(x).encode()))\n",
    "    df.to_csv('data/encrypted_breast_cancer.csv', index=False)\n",
    "    print(\"Data ingestion completed. Encrypted data saved to 'data/encrypted_breast_cancer.csv'.\")\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "def data_preprocessing():\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    df = pd.read_csv('data/encrypted_breast_cancer.csv')\n",
    "\n",
    "    # Decrypt the target column\n",
    "    key = Fernet.generate_key()\n",
    "    cipher_suite = Fernet(key)\n",
    "    df['target'] = df['target'].apply(lambda x: int(cipher_suite.decrypt(eval(x)).decode()))\n",
    "\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Save preprocessed data\n",
    "    pd.DataFrame(X_train).to_csv('data/X_train.csv', index=False)\n",
    "    pd.DataFrame(X_test).to_csv('data/X_test.csv', index=False)\n",
    "    pd.DataFrame(y_train, columns=['target']).to_csv('data/y_train.csv', index=False)\n",
    "    pd.DataFrame(y_test, columns=['target']).to_csv('data/y_test.csv', index=False)\n",
    "\n",
    "    # Save the scaler\n",
    "    joblib.dump(scaler, 'results/scaler.pkl')\n",
    "    print(\"Data preprocessing completed. Preprocessed data and scaler saved.\")\n",
    "\n",
    "# Step 3: Model Training with Encryption\n",
    "def model_training():\n",
    "    print(\"Starting model training...\")\n",
    "    X_train = pd.read_csv('data/X_train.csv')\n",
    "    y_train = pd.read_csv('data/y_train.csv')\n",
    "\n",
    "    # Convert y_train to a 1D array\n",
    "    y_train = y_train.values.ravel()\n",
    "\n",
    "    # Train the model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Encrypt and save the trained model\n",
    "    key = Fernet.generate_key()\n",
    "    cipher_suite = Fernet(key)\n",
    "    joblib.dump(model, 'results/model.pkl')\n",
    "    with open('results/model.pkl', 'rb') as f:\n",
    "        model_data = f.read()\n",
    "    encrypted_model = cipher_suite.encrypt(model_data)\n",
    "    with open('results/encrypted_model.pkl', 'wb') as f:\n",
    "        f.write(encrypted_model)\n",
    "    print(\"Model training completed. Encrypted model saved to 'results/encrypted_model.pkl'.\")\n",
    "\n",
    "# Step 4: Model Evaluation with Logging\n",
    "def model_evaluation():\n",
    "    print(\"Starting model evaluation...\")\n",
    "    X_test = pd.read_csv('data/X_test.csv')\n",
    "    y_test = pd.read_csv('data/y_test.csv')\n",
    "\n",
    "    # Convert y_test to a 1D array\n",
    "    y_test = y_test.values.ravel()\n",
    "\n",
    "    # Load the model\n",
    "    key = Fernet.generate_key()\n",
    "    cipher_suite = Fernet(key)\n",
    "    with open('results/encrypted_model.pkl', 'rb') as f:\n",
    "        encrypted_model = f.read()\n",
    "    decrypted_model = cipher_suite.decrypt(encrypted_model)\n",
    "    with open('results/decrypted_model.pkl', 'wb') as f:\n",
    "        f.write(decrypted_model)\n",
    "    model = joblib.load('results/decrypted_model.pkl')\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model Accuracy: {accuracy}\")\n",
    "\n",
    "    # Log the accuracy\n",
    "    logging.basicConfig(filename='pipeline_audit.log', level=logging.INFO)\n",
    "    logging.info(f\"Model Accuracy: {accuracy}\")\n",
    "\n",
    "    # Save the accuracy result\n",
    "    with open('results/accuracy.txt', 'w') as f:\n",
    "        f.write(f\"Accuracy: {accuracy}\")\n",
    "    print(\"Model evaluation completed. Accuracy saved to 'results/accuracy.txt'.\")\n",
    "\n",
    "# Define the DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2023, 10, 1),\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'ml_pipeline_dag',\n",
    "    default_args=default_args,\n",
    "    description='End-to-End ML Pipeline with Security and Compliance',\n",
    "    schedule_interval='@daily',  # Run the DAG daily\n",
    "    catchup=False,  # Disable catchup to avoid backfilling\n",
    ")\n",
    "\n",
    "# Define tasks\n",
    "ingestion_task = PythonOperator(\n",
    "    task_id='data_ingestion',\n",
    "    python_callable=data_ingestion,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "preprocessing_task = PythonOperator(\n",
    "    task_id='data_preprocessing',\n",
    "    python_callable=data_preprocessing,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "training_task = PythonOperator(\n",
    "    task_id='model_training',\n",
    "    python_callable=model_training,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "evaluation_task = PythonOperator(\n",
    "    task_id='model_evaluation',\n",
    "    python_callable=model_evaluation,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define task dependencies\n",
    "ingestion_task >> preprocessing_task >> training_task >> evaluation_task\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
